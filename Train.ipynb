{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, Input, Sequential, optimizers, callbacks, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from internal_methods import spectrogramFromFile\n",
    "from KerasGenerator import SpectrogramGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_data_dir: str = \"data/wake\"\n",
    "background_data_dir: str = \"data/background\"\n",
    "\n",
    "\n",
    "sr: int = 44100\n",
    "seconds: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.path.join(wake_data_dir, os.listdir(wake_data_dir)[0])\n",
    "sample_shape = spectrogramFromFile(\n",
    "    audio_filepath=sample_path, expand_last_dim=True\n",
    ").shape\n",
    "sample_shape\n",
    "\n",
    "n_samples = len(os.listdir(wake_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 1 : Wake word\n",
    "wake_word_filenames = [\n",
    "    os.path.join(wake_data_dir, item) for item in os.listdir(wake_data_dir)\n",
    "]\n",
    "background_filenames = [\n",
    "    os.path.join(background_data_dir, item) for item in os.listdir(background_data_dir)\n",
    "]\n",
    "\n",
    "random.shuffle(wake_word_filenames)\n",
    "random.shuffle(background_filenames)\n",
    "\n",
    "wake_word_filenames = wake_word_filenames[:n_samples]\n",
    "background_filenames = background_filenames[:n_samples]\n",
    "\n",
    "wake_word_labels = np.full(shape=n_samples, fill_value=1)\n",
    "background_labels = np.full(shape=n_samples, fill_value=0)\n",
    "\n",
    "# ---------------------- #\n",
    "\n",
    "all_filenames = np.concatenate([wake_word_filenames, background_filenames])\n",
    "all_labels = np.concatenate([wake_word_labels, background_labels])\n",
    "\n",
    "label_map = dict(zip(all_filenames, all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    all_filenames, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Generator Initialization\n",
    "params = {\n",
    "    \"sample_shape\": sample_shape,\n",
    "    \"batch_size\": 32,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "train_Generator = SpectrogramGenerator(list_IDs=X_train, label_map=label_map, **params)\n",
    "validation_Generator = SpectrogramGenerator(list_IDs=X_val, label_map=label_map, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0  # Choose any index within the range [0, len(data_generator))\n",
    "\n",
    "# Generate a batch of data\n",
    "X_batch, y_batch = validation_Generator.__getitem__(index)\n",
    "# Print the shapes of the generated batch\n",
    "print(\"X_batch shape:\", X_batch.shape)\n",
    "print(\"y_batch shape:\", y_batch.shape)\n",
    "print(\"y_batch: \", y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Building\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 70\n",
    "batch_size = params[\"batch_size\"]\n",
    "input_shape = params[\"sample_shape\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters:int, kernel_size:int|tuple[int]=3, strides:int|tuple[int]=1, activation:str=\"relu\",padding:str=\"same\" ):\n",
    "    y = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(activation)(y)\n",
    "    y = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=1, padding=padding)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    \n",
    "    print(x.shape, y.shape)\n",
    "    if x.shape[-1] != filters:\n",
    "        # Use pointwise convolution to manipulate filter number without changing dimenstions of spatial data\n",
    "        x = layers.Conv2D(filters=filters, kernel_size=1, strides=strides, padding=padding)(x)\n",
    "    \n",
    "    out = layers.Add()([x, y]) # Skip Connection\n",
    "    out = layers.Activation(activation)(out)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, batch_size=32):\n",
    "    inputs = Input(shape=input_shape, batch_size=batch_size)\n",
    "\n",
    "    x = layers.Conv2D(filters=32, kernel_size=3, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=64)\n",
    "    x = residual_block(x, filters=128)\n",
    "    x = residual_block(x, filters=256, strides=2)\n",
    "    x = residual_block(x, filters=512, strides=2)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.Flatten())(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        units=256, return_sequences=True, kernel_regularizer=regularizers.l2(0.001)\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        units=512, return_sequences=True, kernel_regularizer=regularizers.l2(0.001)\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.LSTM(units=512, kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(units=128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(input_shape=input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build__old_model(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=input_shape, batch_size=batch_size))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Reshape the output to be compatible with LSTM\n",
    "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "\n",
    "    # LSTM layers for temporal dependencies\n",
    "    model.add(\n",
    "        layers.LSTM(\n",
    "            units=512, return_sequences=True, kernel_regularizer=regularizers.l2(0.01)\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(\n",
    "        layers.LSTM(\n",
    "            units=512, return_sequences=True, kernel_regularizer=regularizers.l2(0.01)\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.LSTM(units=512, kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "    # Dense layers for final classification\n",
    "    model.add(layers.Dense(units=128, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = build_model(input_shape=input_shape)\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Input shape: \", input_shape)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    \n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = \"val_loss\"\n",
    "mode = \"min\"\n",
    "verbose = 1\n",
    "\n",
    "early_stopping_callback = callbacks.EarlyStopping(\n",
    "    monitor=monitor, mode=mode, patience=15, restore_best_weights=True, verbose=verbose\n",
    ")\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor=monitor,\n",
    "    mode=mode,\n",
    "    save_best_only=True,\n",
    "    verbose=verbose,\n",
    ")\n",
    "reduce_lr_callback = callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor, mode=mode, factor=0.5, patience=2, min_lr=1e-7, verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"WAKE_WORD_ENHANCED.keras\"\n",
    "\n",
    "history = model.fit(\n",
    "    train_Generator,\n",
    "    validation_data=validation_Generator,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_Generator),\n",
    "    callbacks=[\n",
    "        early_stopping_callback,\n",
    "        checkpoint_callback,\n",
    "        reduce_lr_callback,\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save(model_path)\n",
    "model.save(\"WAKE_WORD_ENHANCED.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(\"accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    validation_Generator, steps=len(validation_Generator)\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 2  # Choose any index within the range [0, len(data_generator))\n",
    "# mapping = []\n",
    "# for i in range(20):\n",
    "\n",
    "#     # Generate a batch of data\n",
    "#     X_batch, y_batch = validation_Generator.__getitem__(i)\n",
    "\n",
    "#     # Print the shapes of the generated batch\n",
    "#     # print(\"X_batch shape:\", X_batch.shape)\n",
    "#     # print(\"y_batch shape:\", y_batch.shape)\n",
    "#     # print(\"y_batch: \", y_batch)\n",
    "\n",
    "#     predictions = _model.predict(X_batch, steps=len(X_batch))\n",
    "#     predictions = (\n",
    "#         np.round(predictions, 1)\n",
    "#         .reshape(\n",
    "#             -1,\n",
    "#         )\n",
    "#         .astype(np.int32)\n",
    "#     )\n",
    "\n",
    "#     mapping.append(all(val == True for val in predictions == y_batch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
